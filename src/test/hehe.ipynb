{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Legal Document Processing, Embedding, and Search\n",
    "This notebook demonstrates how to process Vietnamese legal documents, create embeddings, and search for relevant information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import torch\n",
    "import numpy as np\n",
    "import re\n",
    "from typing import List, Dict, Any, Optional\n",
    "from dataclasses import dataclass, field, asdict\n",
    "from llama_index.core import Document\n",
    "from llama_index.core import StorageContext\n",
    "from llama_index.core import VectorStoreIndex\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "from llama_index.vector_stores.weaviate import WeaviateVectorStore\n",
    "from pyvi import ViTokenizer\n",
    "import weaviate\n",
    "from weaviate.classes.init import Auth\n",
    "from sentence_transformers import SentenceTransformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Legal Document Processing Classes\n",
    "These classes handle the specialized processing of legal documents, including chunking, metadata extraction, and embedding creation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class LegalReference:\n",
    "    \"\"\"Lưu thông tin về tham chiếu pháp lý.\"\"\"\n",
    "    article: Optional[str] = None\n",
    "    paragraph: Optional[str] = None\n",
    "    point: Optional[str] = None\n",
    "    is_current_article: bool = False\n",
    "    raw_text: str = \"\"\n",
    "\n",
    "@dataclass\n",
    "class ChunkMetadata:\n",
    "    \"\"\"Metadata cho chunk pháp lý.\"\"\"\n",
    "    article_id: str\n",
    "    title: str = \"\"\n",
    "    type: str = \"article\"  # article, article_part, paragraph\n",
    "    paragraphs: List[str] = field(default_factory=list)\n",
    "    points: List[str] = field(default_factory=list)\n",
    "    references: List[LegalReference] = field(default_factory=list)\n",
    "\n",
    "@dataclass\n",
    "class LegalChunk:\n",
    "    \"\"\"Một đoạn văn bản pháp lý với metadata.\"\"\"\n",
    "    text: str\n",
    "    metadata: ChunkMetadata\n",
    "    embedding: Optional[np.ndarray] = None\n",
    "\n",
    "class LegalDocument:\n",
    "    \"\"\"Xử lý tài liệu pháp lý để chunking và embedding.\"\"\"\n",
    "    \n",
    "    def __init__(self, max_chunk_tokens=500, overlap_tokens=100):\n",
    "        self.max_chunk_tokens = max_chunk_tokens\n",
    "        self.overlap_tokens = overlap_tokens\n",
    "        self.reference_patterns = {\n",
    "            \"other_article\": re.compile(r'(điểm|khoản) ([a-z]|\\d+)(?:,\\s*(điểm|khoản) ([a-z]|\\d+))* (Điều|khoản) (\\d+)', re.IGNORECASE),\n",
    "            \"same_article\": re.compile(r'(điểm|khoản) ([a-z]|\\d+)(?:,\\s*(điểm|khoản) ([a-z]|\\d+))* (Điều này)', re.IGNORECASE),\n",
    "            \"article_only\": re.compile(r'Điều (\\d+)', re.IGNORECASE)\n",
    "        }\n",
    "    \n",
    "    def process(self, text: str) -> List[LegalChunk]:\n",
    "        \"\"\"Xử lý văn bản pháp lý thành các chunk.\"\"\"\n",
    "        # Phân tách văn bản thành các điều\n",
    "        articles = re.split(r'(Điều \\d+\\.)', text)\n",
    "        chunks = []\n",
    "        \n",
    "        for i in range(1, len(articles), 2):\n",
    "            article_title = articles[i].strip()\n",
    "            article_content = articles[i+1].strip() if i+1 < len(articles) else \"\"\n",
    "            \n",
    "            # Trích xuất số điều\n",
    "            article_id = re.search(r'Điều (\\d+)\\.', article_title).group(1)\n",
    "            \n",
    "            # Trích xuất tiêu đề điều\n",
    "            title_match = re.search(r'^([^0-9\\.\\n]*)(?=\\d+\\.|$)', article_content)\n",
    "            title = title_match.group(1).strip() if title_match else \"\"\n",
    "            \n",
    "            # Quyết định phương pháp phân chunk\n",
    "            article_chunks = self._process_article(\n",
    "                article_title, article_id, title, article_content\n",
    "            )\n",
    "            chunks.extend(article_chunks)\n",
    "        \n",
    "        return chunks\n",
    "    \n",
    "    def _process_article(self, article_title, article_id, title, article_content) -> List[LegalChunk]:\n",
    "        \"\"\"Xử lý một điều thành các chunk.\"\"\"\n",
    "        # Tìm các tham chiếu trong điều\n",
    "        references = self._extract_references(article_content, article_id)\n",
    "        \n",
    "        # Đếm tokens (ước tính theo số từ)\n",
    "        token_count = len(article_content.split())\n",
    "        \n",
    "        if token_count <= self.max_chunk_tokens:\n",
    "            # Điều ngắn: giữ nguyên toàn bộ\n",
    "            metadata = ChunkMetadata(\n",
    "                article_id=article_id,\n",
    "                title=title,\n",
    "                type=\"article\",\n",
    "                references=references\n",
    "            )\n",
    "            \n",
    "            return [LegalChunk(\n",
    "                text=article_title + \" \" + article_content,\n",
    "                metadata=metadata\n",
    "            )]\n",
    "        else:\n",
    "            # Điều dài: phân theo khoản\n",
    "            return self._split_by_paragraphs(\n",
    "                article_title, article_id, title, article_content, references\n",
    "            )\n",
    "    \n",
    "    def _split_by_paragraphs(\n",
    "        self, article_title, article_id, title, article_content, references\n",
    "    ) -> List[LegalChunk]:\n",
    "        \"\"\"Phân tách điều thành các chunk theo khoản.\"\"\"\n",
    "        chunks = []\n",
    "        \n",
    "        # Tách khoản\n",
    "        paragraphs = re.split(r'(\\d+\\.)', article_content)\n",
    "        current_chunk_text = article_title + \" \" + title\n",
    "        current_paragraphs = []\n",
    "        current_refs = []\n",
    "        \n",
    "        for j in range(1, len(paragraphs), 2):\n",
    "            if j+1 < len(paragraphs):\n",
    "                para_num = paragraphs[j].strip().replace(\".\", \"\")\n",
    "                para_content = paragraphs[j+1].strip()\n",
    "                \n",
    "                # Tìm tham chiếu trong khoản\n",
    "                para_refs = self._extract_references(para_content, article_id)\n",
    "                \n",
    "                # Kiểm tra độ dài sau khi thêm khoản mới\n",
    "                candidate = current_chunk_text + \" \" + para_num + \". \" + para_content\n",
    "                candidate_tokens = len(candidate.split())\n",
    "                \n",
    "                if candidate_tokens <= self.max_chunk_tokens:\n",
    "                    # Thêm khoản vào chunk hiện tại\n",
    "                    current_chunk_text = candidate\n",
    "                    current_paragraphs.append(para_num)\n",
    "                    current_refs.extend(para_refs)\n",
    "                else:\n",
    "                    # Lưu chunk hiện tại\n",
    "                    metadata = ChunkMetadata(\n",
    "                        article_id=article_id,\n",
    "                        title=title,\n",
    "                        type=\"article_part\",\n",
    "                        paragraphs=current_paragraphs.copy(),\n",
    "                        references=current_refs.copy()\n",
    "                    )\n",
    "                    \n",
    "                    chunks.append(LegalChunk(\n",
    "                        text=current_chunk_text,\n",
    "                        metadata=metadata\n",
    "                    ))\n",
    "                    \n",
    "                    # Bắt đầu chunk mới với bối cảnh\n",
    "                    # Thêm tiêu đề điều và khoản này\n",
    "                    current_chunk_text = f\"{article_title} (tiếp) {para_num}. {para_content}\"\n",
    "                    current_paragraphs = [para_num]\n",
    "                    current_refs = para_refs.copy()\n",
    "        \n",
    "        # Thêm chunk cuối cùng\n",
    "        if current_paragraphs:\n",
    "            metadata = ChunkMetadata(\n",
    "                article_id=article_id,\n",
    "                title=title,\n",
    "                type=\"article_part\",\n",
    "                paragraphs=current_paragraphs,\n",
    "                references=current_refs\n",
    "            )\n",
    "            \n",
    "            chunks.append(LegalChunk(\n",
    "                text=current_chunk_text,\n",
    "                metadata=metadata\n",
    "            ))\n",
    "        \n",
    "        return chunks\n",
    "    \n",
    "    def _extract_references(self, text: str, current_article_id: str) -> List[LegalReference]:\n",
    "        \"\"\"Trích xuất tham chiếu từ văn bản.\"\"\"\n",
    "        references = []\n",
    "        \n",
    "        # Tìm tham chiếu đến điều khác\n",
    "        for match in self.reference_patterns[\"other_article\"].finditer(text):\n",
    "            ref = LegalReference(\n",
    "                article=match.group(6),\n",
    "                paragraph=match.group(2) if match.group(1).lower() == \"khoản\" else None,\n",
    "                point=match.group(2) if match.group(1).lower() == \"điểm\" else None,\n",
    "                raw_text=match.group(0)\n",
    "            )\n",
    "            references.append(ref)\n",
    "        \n",
    "        # Tìm tham chiếu trong cùng điều\n",
    "        for match in self.reference_patterns[\"same_article\"].finditer(text):\n",
    "            ref = LegalReference(\n",
    "                article=current_article_id,\n",
    "                paragraph=match.group(2) if match.group(1).lower() == \"khoản\" else None,\n",
    "                point=match.group(2) if match.group(1).lower() == \"điểm\" else None,\n",
    "                is_current_article=True,\n",
    "                raw_text=match.group(0)\n",
    "            )\n",
    "            references.append(ref)\n",
    "        \n",
    "        # Tìm tham chiếu chỉ đến điều\n",
    "        for match in self.reference_patterns[\"article_only\"].finditer(text):\n",
    "            article_id = match.group(1)\n",
    "            # Tránh trùng lặp với các điều đã tìm thấy\n",
    "            if not any(r.article == article_id and r.paragraph is None and r.point is None for r in references):\n",
    "                ref = LegalReference(\n",
    "                    article=article_id,\n",
    "                    raw_text=match.group(0)\n",
    "                )\n",
    "                references.append(ref)\n",
    "        \n",
    "        return references\n",
    "\n",
    "class LegalEmbedding:\n",
    "    \"\"\"Tạo và quản lý embedding cho tài liệu pháp lý.\"\"\"\n",
    "    \n",
    "    def __init__(self, model_name='paraphrase-multilingual-MiniLM-L12-v2'):\n",
    "        self.model = SentenceTransformer(model_name, trust_remote_code=True)\n",
    "    \n",
    "    def create_embeddings(self, chunks: List[LegalChunk], method='enhanced') -> List[LegalChunk]:\n",
    "        \"\"\"Tạo embedding cho các chunk theo phương pháp chỉ định.\"\"\"\n",
    "        if method == 'basic':\n",
    "            return self._create_basic_embeddings(chunks)\n",
    "        elif method == 'enhanced':\n",
    "            return self._create_enhanced_embeddings(chunks)\n",
    "        elif method == 'hierarchical':\n",
    "            return self._create_hierarchical_embeddings(chunks)\n",
    "        else:\n",
    "            raise ValueError(f\"Phương pháp embedding không hợp lệ: {method}\")\n",
    "    \n",
    "    def _create_basic_embeddings(self, chunks: List[LegalChunk]) -> List[LegalChunk]:\n",
    "        \"\"\"Tạo embedding cơ bản cho các chunk.\"\"\"\n",
    "        texts = [chunk.text for chunk in chunks]\n",
    "        embeddings = self.model.encode(texts)\n",
    "        \n",
    "        for i, chunk in enumerate(chunks):\n",
    "            chunk.embedding = embeddings[i]\n",
    "        \n",
    "        return chunks\n",
    "    \n",
    "    def _create_enhanced_embeddings(self, chunks: List[LegalChunk]) -> List[LegalChunk]:\n",
    "        \"\"\"Tạo embedding nâng cao với tính năng pháp lý.\"\"\"\n",
    "        \n",
    "        for chunk in chunks:\n",
    "            # 1. Tạo văn bản nâng cao với trọng số cho các phần quan trọng\n",
    "            enhanced_text = chunk.text\n",
    "            \n",
    "            # Tăng cường tiêu đề điều và số điều\n",
    "            article_title_match = re.search(r'(Điều \\d+\\.\\s*[^\\.\\']+)', enhanced_text)\n",
    "            if article_title_match:\n",
    "                article_title = article_title_match.group(1)\n",
    "                # Lặp lại tiêu đề 2 lần để tăng trọng số trong embedding\n",
    "                enhanced_text = article_title + \" \" + article_title + \" \" + enhanced_text\n",
    "            \n",
    "            # Tăng cường số khoản, điểm\n",
    "            para_points = re.findall(r'(\\d+\\.\\s*[^\\.\\']+|[a-z]\\)\\s*[^;]+)', enhanced_text)\n",
    "            if para_points:\n",
    "                # Thêm các khoản, điểm vào đầu văn bản để tăng trọng số\n",
    "                para_points_text = \" \".join(para_points[:3])  # Giới hạn 3 khoản/điểm đầu tiên\n",
    "                enhanced_text = para_points_text + \" \" + enhanced_text\n",
    "            \n",
    "            # 2. Tạo embedding cho văn bản đã tăng cường\n",
    "            chunk.embedding = self.model.encode(enhanced_text)\n",
    "        \n",
    "        return chunks\n",
    "    \n",
    "    def _create_hierarchical_embeddings(self, chunks: List[LegalChunk]) -> List[LegalChunk]:\n",
    "        \"\"\"Tạo embedding phân cấp cho các chunk.\"\"\"\n",
    "        \n",
    "        # Tổ chức chunks theo Điều\n",
    "        article_dict = {}\n",
    "        for chunk in chunks:\n",
    "            article_id = chunk.metadata.article_id\n",
    "            if article_id not in article_dict:\n",
    "                article_dict[article_id] = []\n",
    "            article_dict[article_id].append(chunk)\n",
    "        \n",
    "        # Tạo embedding cấp Điều\n",
    "        article_embeddings = {}\n",
    "        for article_id, article_chunks in article_dict.items():\n",
    "            # Ghép tất cả chunk của điều này\n",
    "            full_article_text = \" \".join([chunk.text for chunk in article_chunks])\n",
    "            \n",
    "            # Tạo embedding cấp Điều\n",
    "            article_embeddings[article_id] = self.model.encode(full_article_text)\n",
    "        \n",
    "        # Kết hợp embedding cấp Điều với embedding cấp chunk\n",
    "        for chunk in chunks:\n",
    "            # Tạo embedding cấp chunk\n",
    "            chunk_embedding = self.model.encode(chunk.text)\n",
    "            \n",
    "            # Kết hợp với embedding cấp Điều (với trọng số)\n",
    "            article_embedding = article_embeddings[chunk.metadata.article_id]\n",
    "            combined_embedding = 0.7 * chunk_embedding + 0.3 * article_embedding\n",
    "            \n",
    "            # Chuẩn hóa\n",
    "            norm = np.linalg.norm(combined_embedding)\n",
    "            if norm > 0:\n",
    "                combined_embedding = combined_embedding / norm\n",
    "            \n",
    "            chunk.embedding = combined_embedding\n",
    "        \n",
    "        return chunks\n",
    "\n",
    "    def search(self, query: str, chunks: List[LegalChunk], top_k: int = 3) -> List[Dict]:\n",
    "        \"\"\"Tìm kiếm các chunk liên quan đến truy vấn.\"\"\"\n",
    "        query_embedding = self.model.encode(query)\n",
    "        \n",
    "        results = []\n",
    "        for chunk in chunks:\n",
    "            if chunk.embedding is not None:\n",
    "                # Tính độ tương đồng cosine\n",
    "                similarity = np.dot(query_embedding, chunk.embedding) / (\n",
    "                    np.linalg.norm(query_embedding) * np.linalg.norm(chunk.embedding)\n",
    "                )\n",
    "                \n",
    "                results.append({\n",
    "                    \"chunk\": chunk,\n",
    "                    \"similarity\": float(similarity)\n",
    "                })\n",
    "        \n",
    "        # Sắp xếp theo độ tương đồng\n",
    "        results.sort(key=lambda x: x[\"similarity\"], reverse=True)\n",
    "        \n",
    "        # Chuyển đổi kết quả sang định dạng dễ sử dụng\n",
    "        formatted_results = []\n",
    "        for result in results[:top_k]:\n",
    "            chunk = result[\"chunk\"]\n",
    "            formatted_results.append({\n",
    "                \"text\": chunk.text,\n",
    "                \"metadata\": asdict(chunk.metadata),\n",
    "                \"similarity\": result[\"similarity\"]\n",
    "            })\n",
    "        \n",
    "        return formatted_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Weaviate configuration\n",
    "WEAVIATE_URL=\"https://ekg9amszqaap6mlmw9fixq.c0.us-west3.gcp.weaviate.cloud\"\n",
    "WEAVIATE_API_KEY=\"WZoR5excldEQt0XqH4E2X3zMEvVfvFzr7lc5\"\n",
    "DATA_COLLECTION = \"ND168\"\n",
    "\n",
    "# Device and model configuration\n",
    "DEVICE = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "MODEL_NAME = \"dangvantuan/vietnamese-document-embedding\"\n",
    "\n",
    "\n",
    "# Chunking configuration\n",
    "MAX_CHUNK_TOKENS = 512  # Optimized for Vietnamese legal text\n",
    "CHUNK_OVERLAP = 50  # Small overlap to maintain context\n",
    "EMBEDDING_METHOD = \"enhanced\"  # Options: basic, enhanced, hierarchical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and Process Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load raw documents\n",
    "with open(\"/home/ltnga/LawVN-Instructction-Gen/src/data/data.json\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Process each document with specialized legal document processing\n",
    "legal_processor = LegalDocument(max_chunk_tokens=MAX_CHUNK_TOKENS, overlap_tokens=CHUNK_OVERLAP)\n",
    "all_chunks = []\n",
    "\n",
    "for doc_text in data:\n",
    "    # Process with legal document chunking\n",
    "    chunks = legal_processor.process(doc_text)\n",
    "    all_chunks.extend(chunks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize embedding generator\n",
    "embedder = LegalEmbedding(model_name=MODEL_NAME)\n",
    "\n",
    "# Generate embeddings with the specified method\n",
    "chunks_with_embeddings = embedder.create_embeddings(all_chunks, method=EMBEDDING_METHOD)\n",
    "\n",
    "# Optional: Apply Vietnamese tokenization for improved search\n",
    "for chunk in chunks_with_embeddings:\n",
    "    # Store original text for display\n",
    "    chunk.metadata.original_text = chunk.text\n",
    "    # Tokenize for better vectorization (Vietnamese-specific)\n",
    "    chunk.tokenized_text = ViTokenizer.tokenize(chunk.text.lower())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Weaviate Vector Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ltnga/LawVN-Instructction-Gen/venv/lib/python3.10/site-packages/weaviate/warnings.py:314: ResourceWarning: Con004: The connection to Weaviate was not closed properly. This can lead to memory leaks.\n",
      "            Please make sure to close the connection using `client.close()`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "WeaviateInvalidInputError",
     "evalue": "Invalid input provided: Invalid collection config create parameters: 5 validation errors for _CollectionConfigCreate\nproperties.0.data_type\n  Input should be an instance of DataType [type=is_instance_of, input_value='text', input_type=str]\n    For further information visit https://errors.pydantic.dev/2.10/v/is_instance_of\nproperties.1.data_type\n  Input should be an instance of DataType [type=is_instance_of, input_value='text', input_type=str]\n    For further information visit https://errors.pydantic.dev/2.10/v/is_instance_of\nproperties.2.data_type\n  Input should be an instance of DataType [type=is_instance_of, input_value='text', input_type=str]\n    For further information visit https://errors.pydantic.dev/2.10/v/is_instance_of\nproperties.3.data_type\n  Input should be an instance of DataType [type=is_instance_of, input_value='text', input_type=str]\n    For further information visit https://errors.pydantic.dev/2.10/v/is_instance_of\nproperties.4.data_type\n  Input should be an instance of DataType [type=is_instance_of, input_value='text[]', input_type=str]\n    For further information visit https://errors.pydantic.dev/2.10/v/is_instance_of.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValidationError\u001b[0m                           Traceback (most recent call last)",
      "File \u001b[0;32m~/LawVN-Instructction-Gen/venv/lib/python3.10/site-packages/weaviate/collections/collections/async_.py:116\u001b[0m, in \u001b[0;36m_CollectionsAsync.create\u001b[0;34m(self, name, description, generative_config, inverted_index_config, multi_tenancy_config, properties, references, replication_config, reranker_config, sharding_config, vector_index_config, vectorizer_config, data_model_properties, data_model_references, skip_argument_validation)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 116\u001b[0m     config \u001b[38;5;241m=\u001b[39m \u001b[43m_CollectionConfigCreate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    117\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdescription\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdescription\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    118\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgenerative_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgenerative_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    119\u001b[0m \u001b[43m        \u001b[49m\u001b[43minverted_index_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minverted_index_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    120\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmulti_tenancy_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmulti_tenancy_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    121\u001b[0m \u001b[43m        \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    122\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproperties\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproperties\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    123\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreferences\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreferences\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    124\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreplication_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreplication_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    125\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreranker_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreranker_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    126\u001b[0m \u001b[43m        \u001b[49m\u001b[43msharding_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msharding_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    127\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvectorizer_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvectorizer_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    128\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvector_index_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvector_index_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    129\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    130\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ValidationError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/LawVN-Instructction-Gen/venv/lib/python3.10/site-packages/pydantic/main.py:214\u001b[0m, in \u001b[0;36mBaseModel.__init__\u001b[0;34m(self, **data)\u001b[0m\n\u001b[1;32m    213\u001b[0m __tracebackhide__ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m--> 214\u001b[0m validated_self \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__pydantic_validator__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalidate_python\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mself_instance\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m validated_self:\n",
      "\u001b[0;31mValidationError\u001b[0m: 5 validation errors for _CollectionConfigCreate\nproperties.0.data_type\n  Input should be an instance of DataType [type=is_instance_of, input_value='text', input_type=str]\n    For further information visit https://errors.pydantic.dev/2.10/v/is_instance_of\nproperties.1.data_type\n  Input should be an instance of DataType [type=is_instance_of, input_value='text', input_type=str]\n    For further information visit https://errors.pydantic.dev/2.10/v/is_instance_of\nproperties.2.data_type\n  Input should be an instance of DataType [type=is_instance_of, input_value='text', input_type=str]\n    For further information visit https://errors.pydantic.dev/2.10/v/is_instance_of\nproperties.3.data_type\n  Input should be an instance of DataType [type=is_instance_of, input_value='text', input_type=str]\n    For further information visit https://errors.pydantic.dev/2.10/v/is_instance_of\nproperties.4.data_type\n  Input should be an instance of DataType [type=is_instance_of, input_value='text[]', input_type=str]\n    For further information visit https://errors.pydantic.dev/2.10/v/is_instance_of",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mWeaviateInvalidInputError\u001b[0m                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[33], line 12\u001b[0m\n\u001b[1;32m      9\u001b[0m     client\u001b[38;5;241m.\u001b[39mcollections\u001b[38;5;241m.\u001b[39mdelete(DATA_COLLECTION)\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# Create collection with proper schema\u001b[39;00m\n\u001b[0;32m---> 12\u001b[0m collection \u001b[38;5;241m=\u001b[39m \u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollections\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mDATA_COLLECTION\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvectorizer_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# We'll provide our own vectors\u001b[39;49;00m\n\u001b[1;32m     15\u001b[0m \u001b[43m    \u001b[49m\u001b[43mproperties\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m        \u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mname\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtext\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdata_type\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtext\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[43m        \u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mname\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43marticle_id\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdata_type\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtext\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[43m        \u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mname\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtitle\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdata_type\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtext\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[43m        \u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mname\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtype\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdata_type\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtext\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[43m        \u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mname\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mparagraphs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdata_type\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtext[]\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m}\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[43m    \u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/LawVN-Instructction-Gen/venv/lib/python3.10/site-packages/weaviate/collections/collections/sync.py:107\u001b[0m, in \u001b[0;36m_Collections.create\u001b[0;34m(self, name, description, generative_config, inverted_index_config, multi_tenancy_config, properties, references, replication_config, reranker_config, sharding_config, vector_index_config, vectorizer_config, data_model_properties, data_model_references, skip_argument_validation)\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate\u001b[39m(\n\u001b[1;32m     37\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m     38\u001b[0m     name: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     55\u001b[0m     skip_argument_validation: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m     56\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Collection[Properties, References]:\n\u001b[1;32m     57\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Use this method to create a collection in Weaviate and immediately return a collection object.\u001b[39;00m\n\u001b[1;32m     58\u001b[0m \n\u001b[1;32m     59\u001b[0m \u001b[38;5;124;03m    This method takes several arguments that allow you to configure the collection to your liking. Each argument\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[38;5;124;03m            If Weaviate reports a non-OK status.\u001b[39;00m\n\u001b[1;32m    106\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 107\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__loop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_until_complete\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    108\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__collections\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    109\u001b[0m \u001b[43m        \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    110\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdescription\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdescription\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    111\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgenerative_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgenerative_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    112\u001b[0m \u001b[43m        \u001b[49m\u001b[43minverted_index_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minverted_index_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    113\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmulti_tenancy_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmulti_tenancy_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    114\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproperties\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproperties\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    115\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreferences\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreferences\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    116\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreplication_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreplication_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    117\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreranker_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreranker_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    118\u001b[0m \u001b[43m        \u001b[49m\u001b[43msharding_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msharding_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    119\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvector_index_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvector_index_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    120\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvectorizer_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvectorizer_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    121\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdata_model_properties\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_model_properties\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    122\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdata_model_references\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_model_references\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    123\u001b[0m \u001b[43m        \u001b[49m\u001b[43mskip_argument_validation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mskip_argument_validation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    124\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    125\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget(\n\u001b[1;32m    126\u001b[0m         name,\n\u001b[1;32m    127\u001b[0m         data_model_properties\u001b[38;5;241m=\u001b[39mdata_model_properties,\n\u001b[1;32m    128\u001b[0m         data_model_references\u001b[38;5;241m=\u001b[39mdata_model_references,\n\u001b[1;32m    129\u001b[0m         skip_argument_validation\u001b[38;5;241m=\u001b[39mskip_argument_validation,\n\u001b[1;32m    130\u001b[0m     )\n",
      "File \u001b[0;32m~/LawVN-Instructction-Gen/venv/lib/python3.10/site-packages/weaviate/event_loop.py:42\u001b[0m, in \u001b[0;36m_EventLoop.run_until_complete\u001b[0;34m(self, f, *args, **kwargs)\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m WeaviateClosedClientError()\n\u001b[1;32m     41\u001b[0m fut \u001b[38;5;241m=\u001b[39m asyncio\u001b[38;5;241m.\u001b[39mrun_coroutine_threadsafe(f(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloop)\n\u001b[0;32m---> 42\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfut\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/lib/python3.10/concurrent/futures/_base.py:451\u001b[0m, in \u001b[0;36mFuture.result\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    449\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n\u001b[1;32m    450\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;241m==\u001b[39m FINISHED:\n\u001b[0;32m--> 451\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__get_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    453\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_condition\u001b[38;5;241m.\u001b[39mwait(timeout)\n\u001b[1;32m    455\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;129;01min\u001b[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n",
      "File \u001b[0;32m/usr/lib/python3.10/concurrent/futures/_base.py:403\u001b[0m, in \u001b[0;36mFuture.__get_result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    401\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception:\n\u001b[1;32m    402\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 403\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception\n\u001b[1;32m    404\u001b[0m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    405\u001b[0m         \u001b[38;5;66;03m# Break a reference cycle with the exception in self._exception\u001b[39;00m\n\u001b[1;32m    406\u001b[0m         \u001b[38;5;28mself\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/LawVN-Instructction-Gen/venv/lib/python3.10/site-packages/weaviate/collections/collections/async_.py:131\u001b[0m, in \u001b[0;36m_CollectionsAsync.create\u001b[0;34m(self, name, description, generative_config, inverted_index_config, multi_tenancy_config, properties, references, replication_config, reranker_config, sharding_config, vector_index_config, vectorizer_config, data_model_properties, data_model_references, skip_argument_validation)\u001b[0m\n\u001b[1;32m    116\u001b[0m     config \u001b[38;5;241m=\u001b[39m _CollectionConfigCreate(\n\u001b[1;32m    117\u001b[0m         description\u001b[38;5;241m=\u001b[39mdescription,\n\u001b[1;32m    118\u001b[0m         generative_config\u001b[38;5;241m=\u001b[39mgenerative_config,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    128\u001b[0m         vector_index_config\u001b[38;5;241m=\u001b[39mvector_index_config,\n\u001b[1;32m    129\u001b[0m     )\n\u001b[1;32m    130\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ValidationError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m--> 131\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m WeaviateInvalidInputError(\n\u001b[1;32m    132\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid collection config create parameters: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    133\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m    134\u001b[0m name \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m_create(config\u001b[38;5;241m.\u001b[39m_to_dict())\n\u001b[1;32m    135\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m (\n\u001b[1;32m    136\u001b[0m     config\u001b[38;5;241m.\u001b[39mname \u001b[38;5;241m==\u001b[39m name\n\u001b[1;32m    137\u001b[0m ), \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mName of created collection (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) does not match given name (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig\u001b[38;5;241m.\u001b[39mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[0;31mWeaviateInvalidInputError\u001b[0m: Invalid input provided: Invalid collection config create parameters: 5 validation errors for _CollectionConfigCreate\nproperties.0.data_type\n  Input should be an instance of DataType [type=is_instance_of, input_value='text', input_type=str]\n    For further information visit https://errors.pydantic.dev/2.10/v/is_instance_of\nproperties.1.data_type\n  Input should be an instance of DataType [type=is_instance_of, input_value='text', input_type=str]\n    For further information visit https://errors.pydantic.dev/2.10/v/is_instance_of\nproperties.2.data_type\n  Input should be an instance of DataType [type=is_instance_of, input_value='text', input_type=str]\n    For further information visit https://errors.pydantic.dev/2.10/v/is_instance_of\nproperties.3.data_type\n  Input should be an instance of DataType [type=is_instance_of, input_value='text', input_type=str]\n    For further information visit https://errors.pydantic.dev/2.10/v/is_instance_of\nproperties.4.data_type\n  Input should be an instance of DataType [type=is_instance_of, input_value='text[]', input_type=str]\n    For further information visit https://errors.pydantic.dev/2.10/v/is_instance_of."
     ]
    }
   ],
   "source": [
    "# Connect to Weaviate\n",
    "client = weaviate.connect_to_weaviate_cloud(\n",
    "    cluster_url=WEAVIATE_URL,\n",
    "    auth_credentials=Auth.api_key(WEAVIATE_API_KEY),\n",
    ")\n",
    "\n",
    "# Check if collection exists, delete if it does\n",
    "if client.collections.exists(DATA_COLLECTION):\n",
    "    client.collections.delete(DATA_COLLECTION)\n",
    "\n",
    "# Create collection with proper schema\n",
    "collection = client.collections.create(\n",
    "    name=DATA_COLLECTION,\n",
    "    vectorizer_config=None,  # We'll provide our own vectors\n",
    "    properties=[\n",
    "        {\"name\": \"text\", \"data_type\": \"text\"},\n",
    "        {\"name\": \"article_id\", \"data_type\": \"text\"},\n",
    "        {\"name\": \"title\", \"data_type\": \"text\"},\n",
    "        {\"name\": \"type\", \"data_type\": \"text\"},\n",
    "        {\"name\": \"paragraphs\", \"data_type\": \"text[]\"}\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Index Documents in Weaviate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare batch import\n",
    "with collection.batch.dynamic() as batch:\n",
    "    for i, chunk in enumerate(chunks_with_embeddings):\n",
    "        # Convert paragraphs list to strings for Weaviate compatibility\n",
    "        paragraphs_str = [str(p) for p in chunk.metadata.paragraphs]\n",
    "        \n",
    "        # Add object with its embedding\n",
    "        batch.add_object(\n",
    "            properties={\n",
    "                \"text\": chunk.text,\n",
    "                \"article_id\": chunk.metadata.article_id,\n",
    "                \"title\": chunk.metadata.title,\n",
    "                \"type\": chunk.metadata.type,\n",
    "                \"paragraphs\": paragraphs_str\n",
    "            },\n",
    "            vector=chunk.embedding.tolist() if chunk.embedding is not None else None\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Query Interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_legal_documents(query, top_k=5):\n",
    "    \"\"\"Search for legal documents matching the query.\"\"\"\n",
    "    # Tokenize query for Vietnamese\n",
    "    tokenized_query = ViTokenizer.tokenize(query.lower())\n",
    "    \n",
    "    # Create query embedding using our legal embedder\n",
    "    query_embedding = embedder.model.encode(tokenized_query)\n",
    "    \n",
    "    # Search in Weaviate\n",
    "    results = collection.query.near_vector(\n",
    "        near_vector=query_embedding.tolist(),\n",
    "        limit=top_k,\n",
    "        return_properties=[\"text\", \"article_id\", \"title\", \"type\", \"paragraphs\"]\n",
    "    )\n",
    "    \n",
    "    return results.objects"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result 1:\n",
      "Article ID: 16\n",
      "Text: Điều 16. Xử phạt, trừ điểm giấy phép lái xe của người điều khiển xe mô tô, xe gắn máy (kể cả xe máy điện), các loại xe tương tự xe mô tô và các loại xe tương tự xe gắn máy vi phạm quy tắc giao thông đường bộ\n",
      "...\n",
      "h) Không đội \"mũ bảo hiểm cho người đi mô tô, xe máy\" hoặc đội \"mũ bảo hiểm cho người đi mô tô, xe máy\" không cài quai đúng quy cách khi điều khiển xe tham gia giao thông trên đường bộ;\n",
      "i) Chở người ngồi trên xe không đội \"mũ bảo hiểm cho người đi mô tô, xe máy\" hoặc đội \"mũ bảo hiểm cho người đi mô tô, xe máy\" không cài quai đúng quy cách, trừ trường hợp chở người bệnh đi cấp cứu, trẻ em dưới 06 tuổi, áp giải người có hành vi vi phạm pháp luật;\n",
      "\n",
      "================================================\n",
      "Result 2:\n",
      "...\n"
     ]
    }
   ],
   "source": [
    "# Test the search\n",
    "query = \"Không đội mũ bảo hiểm thì bị phạt bao nhiêu tiền?\"\n",
    "results = search_legal_documents(query)\n",
    "\n",
    "for i, result in enumerate(results):\n",
    "    print(f\"Result {i+1}:\")\n",
    "    print(f\"Article ID: {result.properties['article_id']}\")\n",
    "    \n",
    "    # Extract and display the relevant part about helmets\n",
    "    text = result.properties[\"text\"]\n",
    "    if \"mũ bảo hiểm\" in text.lower():\n",
    "        # Display the article headline\n",
    "        headline = text.split(\"\\n\")[0] if \"\\n\" in text else text[:100]\n",
    "        print(f\"Text: {headline}\")\n",
    "        \n",
    "        # Find and display paragraphs containing the helmet info\n",
    "        for paragraph in text.split(\"\\n\"):\n",
    "            if \"mũ bảo hiểm\" in paragraph.lower():\n",
    "                print(f\"...\\n{paragraph}\")\n",
    "    else:\n",
    "        # Just show a preview if helmet info isn't explicitly mentioned\n",
    "        print(f\"Text: {text[:200]}...\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 48)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced Search With Specific Reference Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_penalty_info(query, top_k=3):\n",
    "    \"\"\"Extract specific penalty information based on the query.\"\"\"\n",
    "    results = search_legal_documents(query, top_k=top_k)\n",
    "    \n",
    "    penalties = []\n",
    "    for result in results:\n",
    "        text = result.properties[\"text\"]\n",
    "        \n",
    "        # Look for penalty amounts\n",
    "        money_patterns = re.findall(r'phạt tiền từ ([\\d\\.]+) đồng đến ([\\d\\.]+) đồng', text, re.IGNORECASE)\n",
    "        \n",
    "        # Extract relevant behavior descriptions\n",
    "        behaviors = []\n",
    "        for line in text.split(\"\\n\"):\n",
    "            if \"mũ bảo hiểm\" in line.lower() and (\"không đội\" in line.lower() or \"không cài quai\" in line.lower()):\n",
    "                behaviors.append(line.strip())\n",
    "        \n",
    "        # If we found both penalty and behavior\n",
    "        if money_patterns and behaviors:\n",
    "            for min_amount, max_amount in money_patterns:\n",
    "                penalties.append({\n",
    "                    \"article_id\": result.properties[\"article_id\"],\n",
    "                    \"min_amount\": min_amount,\n",
    "                    \"max_amount\": max_amount,\n",
    "                    \"behaviors\": behaviors,\n",
    "                    \"excerpt\": text[:300] + \"...\" if len(text) > 300 else text\n",
    "                })\n",
    "    \n",
    "    return penalties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== MỨC PHẠT TIỀN KHI KHÔNG ĐỘI MŨ BẢO HIỂM ===\n",
      "\n",
      "Theo Điều 16:\n",
      "- Mức phạt: 400.000 đồng đến 600.000 đồng\n",
      "- Áp dụng cho hành vi:\n",
      "  * Không đội \"mũ bảo hiểm cho người đi mô tô, xe máy\" hoặc đội \"mũ bảo hiểm cho người đi mô tô, xe máy\" không cài quai đúng quy cách khi điều khiển xe tham gia giao thông trên đường bộ\n",
      "  * Chở người ngồi trên xe không đội \"mũ bảo hiểm cho người đi mô tô, xe máy\" hoặc đội \"mũ bảo hiểm cho người đi mô tô, xe máy\" không cài quai đúng quy cách, trừ trường hợp chở người bệnh đi cấp cứu, trẻ em dưới 06 tuổi, áp giải người có hành vi vi phạm pháp luật\n"
     ]
    }
   ],
   "source": [
    "# Extract specific penalty information\n",
    "query = \"Không đội mũ bảo hiểm khi đi xe máy mức phạt\"\n",
    "penalty_info = extract_penalty_info(query)\n",
    "\n",
    "# Format the results\n",
    "print(\"=== MỨC PHẠT TIỀN KHI KHÔNG ĐỘI MŨ BẢO HIỂM ===\")\n",
    "print()\n",
    "\n",
    "for penalty in penalty_info:\n",
    "    print(f\"Theo Điều {penalty['article_id']}:\")\n",
    "    print(f\"- Mức phạt: {penalty['min_amount']} đồng đến {penalty['max_amount']} đồng\")\n",
    "    print(\"- Áp dụng cho hành vi:\")\n",
    "    for behavior in penalty['behaviors']:\n",
    "        print(f\"  * {behavior}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
