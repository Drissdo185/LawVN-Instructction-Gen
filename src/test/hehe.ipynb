{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Legal Document Processing, Embedding, and Search\n",
    "This notebook demonstrates how to process Vietnamese legal documents, create embeddings, and search for relevant information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import torch\n",
    "import numpy as np\n",
    "import re\n",
    "from typing import List, Dict, Any, Optional\n",
    "from dataclasses import dataclass, field, asdict\n",
    "from llama_index.core import Document\n",
    "from llama_index.core import StorageContext\n",
    "from llama_index.core import VectorStoreIndex\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "from llama_index.vector_stores.weaviate import WeaviateVectorStore\n",
    "from pyvi import ViTokenizer\n",
    "import weaviate\n",
    "from weaviate.classes.init import Auth\n",
    "# Import DataType enum for Weaviate schema definition\n",
    "from weaviate.classes.config import DataType\n",
    "from sentence_transformers import SentenceTransformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Legal Document Processing Classes\n",
    "These classes handle the specialized processing of legal documents, including chunking, metadata extraction, and embedding creation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class LegalReference:\n",
    "    \"\"\"Lưu thông tin về tham chiếu pháp lý.\"\"\"\n",
    "    article: Optional[str] = None\n",
    "    paragraph: Optional[str] = None\n",
    "    point: Optional[str] = None\n",
    "    is_current_article: bool = False\n",
    "    raw_text: str = \"\"\n",
    "\n",
    "@dataclass\n",
    "class ChunkMetadata:\n",
    "    \"\"\"Metadata cho chunk pháp lý.\"\"\"\n",
    "    article_id: str\n",
    "    title: str = \"\"\n",
    "    type: str = \"article\"  # article, article_part, paragraph\n",
    "    paragraphs: List[str] = field(default_factory=list)\n",
    "    points: List[str] = field(default_factory=list)\n",
    "    references: List[LegalReference] = field(default_factory=list)\n",
    "\n",
    "@dataclass\n",
    "class LegalChunk:\n",
    "    \"\"\"Một đoạn văn bản pháp lý với metadata.\"\"\"\n",
    "    text: str\n",
    "    metadata: ChunkMetadata\n",
    "    embedding: Optional[np.ndarray] = None\n",
    "\n",
    "class LegalDocument:\n",
    "    \"\"\"Xử lý tài liệu pháp lý để chunking và embedding.\"\"\"\n",
    "    \n",
    "    def __init__(self, max_chunk_tokens=1000, overlap_tokens=300):\n",
    "        self.max_chunk_tokens = max_chunk_tokens\n",
    "        self.overlap_tokens = overlap_tokens\n",
    "        self.reference_patterns = {\n",
    "            \"other_article\": re.compile(r'(điểm|khoản) ([a-z]|\\d+)(?:,\\s*(điểm|khoản) ([a-z]|\\d+))* (Điều|khoản) (\\d+)', re.IGNORECASE),\n",
    "            \"same_article\": re.compile(r'(điểm|khoản) ([a-z]|\\d+)(?:,\\s*(điểm|khoản) ([a-z]|\\d+))* (Điều này)', re.IGNORECASE),\n",
    "            \"article_only\": re.compile(r'Điều (\\d+)', re.IGNORECASE)\n",
    "        }\n",
    "    \n",
    "    def process(self, text: str) -> List[LegalChunk]:\n",
    "        \"\"\"Xử lý văn bản pháp lý thành các chunk.\"\"\"\n",
    "        # Phân tách văn bản thành các điều\n",
    "        articles = re.split(r'(Điều \\d+\\.)', text)\n",
    "        chunks = []\n",
    "        \n",
    "        for i in range(1, len(articles), 2):\n",
    "            article_title = articles[i].strip()\n",
    "            article_content = articles[i+1].strip() if i+1 < len(articles) else \"\"\n",
    "            \n",
    "            # Trích xuất số điều\n",
    "            article_id = re.search(r'Điều (\\d+)\\.', article_title).group(1)\n",
    "            \n",
    "            # Trích xuất tiêu đề điều\n",
    "            title_match = re.search(r'^([^0-9\\.\\n]*)(?=\\d+\\.|$)', article_content)\n",
    "            title = title_match.group(1).strip() if title_match else \"\"\n",
    "            \n",
    "            # Quyết định phương pháp phân chunk\n",
    "            article_chunks = self._process_article(\n",
    "                article_title, article_id, title, article_content\n",
    "            )\n",
    "            chunks.extend(article_chunks)\n",
    "        \n",
    "        return chunks\n",
    "    \n",
    "    def _process_article(self, article_title, article_id, title, article_content) -> List[LegalChunk]:\n",
    "        \"\"\"Xử lý một điều thành các chunk.\"\"\"\n",
    "        # Tìm các tham chiếu trong điều\n",
    "        references = self._extract_references(article_content, article_id)\n",
    "        \n",
    "        # Đếm tokens (ước tính theo số từ)\n",
    "        token_count = len(article_content.split())\n",
    "        \n",
    "        if token_count <= self.max_chunk_tokens:\n",
    "            # Điều ngắn: giữ nguyên toàn bộ\n",
    "            metadata = ChunkMetadata(\n",
    "                article_id=article_id,\n",
    "                title=title,\n",
    "                type=\"article\",\n",
    "                references=references\n",
    "            )\n",
    "            \n",
    "            return [LegalChunk(\n",
    "                text=article_title + \" \" + article_content,\n",
    "                metadata=metadata\n",
    "            )]\n",
    "        else:\n",
    "            # Điều dài: phân theo khoản\n",
    "            return self._split_by_paragraphs(\n",
    "                article_title, article_id, title, article_content, references\n",
    "            )\n",
    "    \n",
    "    def _split_by_paragraphs(\n",
    "        self, article_title, article_id, title, article_content, references\n",
    "    ) -> List[LegalChunk]:\n",
    "        \"\"\"Phân tách điều thành các chunk theo khoản.\"\"\"\n",
    "        chunks = []\n",
    "        \n",
    "        # Tách khoản\n",
    "        paragraphs = re.split(r'(\\d+\\.)', article_content)\n",
    "        current_chunk_text = article_title + \" \" + title\n",
    "        current_paragraphs = []\n",
    "        current_refs = []\n",
    "        \n",
    "        for j in range(1, len(paragraphs), 2):\n",
    "            if j+1 < len(paragraphs):\n",
    "                para_num = paragraphs[j].strip().replace(\".\", \"\")\n",
    "                para_content = paragraphs[j+1].strip()\n",
    "                \n",
    "                # Tìm tham chiếu trong khoản\n",
    "                para_refs = self._extract_references(para_content, article_id)\n",
    "                \n",
    "                # Kiểm tra độ dài sau khi thêm khoản mới\n",
    "                candidate = current_chunk_text + \" \" + para_num + \". \" + para_content\n",
    "                candidate_tokens = len(candidate.split())\n",
    "                \n",
    "                if candidate_tokens <= self.max_chunk_tokens:\n",
    "                    # Thêm khoản vào chunk hiện tại\n",
    "                    current_chunk_text = candidate\n",
    "                    current_paragraphs.append(para_num)\n",
    "                    current_refs.extend(para_refs)\n",
    "                else:\n",
    "                    # Lưu chunk hiện tại\n",
    "                    metadata = ChunkMetadata(\n",
    "                        article_id=article_id,\n",
    "                        title=title,\n",
    "                        type=\"article_part\",\n",
    "                        paragraphs=current_paragraphs.copy(),\n",
    "                        references=current_refs.copy()\n",
    "                    )\n",
    "                    \n",
    "                    chunks.append(LegalChunk(\n",
    "                        text=current_chunk_text,\n",
    "                        metadata=metadata\n",
    "                    ))\n",
    "                    \n",
    "                    # Bắt đầu chunk mới với bối cảnh\n",
    "                    # Thêm tiêu đề điều và khoản này\n",
    "                    current_chunk_text = f\"{article_title} (tiếp) {para_num}. {para_content}\"\n",
    "                    current_paragraphs = [para_num]\n",
    "                    current_refs = para_refs.copy()\n",
    "        \n",
    "        # Thêm chunk cuối cùng\n",
    "        if current_paragraphs:\n",
    "            metadata = ChunkMetadata(\n",
    "                article_id=article_id,\n",
    "                title=title,\n",
    "                type=\"article_part\",\n",
    "                paragraphs=current_paragraphs,\n",
    "                references=current_refs\n",
    "            )\n",
    "            \n",
    "            chunks.append(LegalChunk(\n",
    "                text=current_chunk_text,\n",
    "                metadata=metadata\n",
    "            ))\n",
    "        \n",
    "        return chunks\n",
    "    \n",
    "    def _extract_references(self, text: str, current_article_id: str) -> List[LegalReference]:\n",
    "        \"\"\"Trích xuất tham chiếu từ văn bản.\"\"\"\n",
    "        references = []\n",
    "        \n",
    "        # Tìm tham chiếu đến điều khác\n",
    "        for match in self.reference_patterns[\"other_article\"].finditer(text):\n",
    "            ref = LegalReference(\n",
    "                article=match.group(6),\n",
    "                paragraph=match.group(2) if match.group(1).lower() == \"khoản\" else None,\n",
    "                point=match.group(2) if match.group(1).lower() == \"điểm\" else None,\n",
    "                raw_text=match.group(0)\n",
    "            )\n",
    "            references.append(ref)\n",
    "        \n",
    "        # Tìm tham chiếu trong cùng điều\n",
    "        for match in self.reference_patterns[\"same_article\"].finditer(text):\n",
    "            ref = LegalReference(\n",
    "                article=current_article_id,\n",
    "                paragraph=match.group(2) if match.group(1).lower() == \"khoản\" else None,\n",
    "                point=match.group(2) if match.group(1).lower() == \"điểm\" else None,\n",
    "                is_current_article=True,\n",
    "                raw_text=match.group(0)\n",
    "            )\n",
    "            references.append(ref)\n",
    "        \n",
    "        # Tìm tham chiếu chỉ đến điều\n",
    "        for match in self.reference_patterns[\"article_only\"].finditer(text):\n",
    "            article_id = match.group(1)\n",
    "            # Tránh trùng lặp với các điều đã tìm thấy\n",
    "            if not any(r.article == article_id and r.paragraph is None and r.point is None for r in references):\n",
    "                ref = LegalReference(\n",
    "                    article=article_id,\n",
    "                    raw_text=match.group(0)\n",
    "                )\n",
    "                references.append(ref)\n",
    "        \n",
    "        return references\n",
    "\n",
    "class LegalEmbedding:\n",
    "    \"\"\"Tạo và quản lý embedding cho tài liệu pháp lý.\"\"\"\n",
    "    \n",
    "    def __init__(self, model_name='paraphrase-multilingual-MiniLM-L12-v2'):\n",
    "        self.model = SentenceTransformer(model_name, trust_remote_code=True)\n",
    "    \n",
    "    def create_embeddings(self, chunks: List[LegalChunk], method='enhanced') -> List[LegalChunk]:\n",
    "        \"\"\"Tạo embedding cho các chunk theo phương pháp chỉ định.\"\"\"\n",
    "        if method == 'basic':\n",
    "            return self._create_basic_embeddings(chunks)\n",
    "        elif method == 'enhanced':\n",
    "            return self._create_enhanced_embeddings(chunks)\n",
    "        elif method == 'hierarchical':\n",
    "            return self._create_hierarchical_embeddings(chunks)\n",
    "        else:\n",
    "            raise ValueError(f\"Phương pháp embedding không hợp lệ: {method}\")\n",
    "    \n",
    "    def _create_basic_embeddings(self, chunks: List[LegalChunk]) -> List[LegalChunk]:\n",
    "        \"\"\"Tạo embedding cơ bản cho các chunk.\"\"\"\n",
    "        texts = [chunk.text for chunk in chunks]\n",
    "        embeddings = self.model.encode(texts)\n",
    "        \n",
    "        for i, chunk in enumerate(chunks):\n",
    "            chunk.embedding = embeddings[i]\n",
    "        \n",
    "        return chunks\n",
    "    \n",
    "    def _create_enhanced_embeddings(self, chunks: List[LegalChunk]) -> List[LegalChunk]:\n",
    "        \"\"\"Tạo embedding nâng cao với tính năng pháp lý.\"\"\"\n",
    "        \n",
    "        for chunk in chunks:\n",
    "            # 1. Tạo văn bản nâng cao với trọng số cho các phần quan trọng\n",
    "            enhanced_text = chunk.text\n",
    "            \n",
    "            # Tăng cường tiêu đề điều và số điều\n",
    "            article_title_match = re.search(r'(Điều \\d+\\.\\s*[^\\.\\']+)', enhanced_text)\n",
    "            if article_title_match:\n",
    "                article_title = article_title_match.group(1)\n",
    "                # Lặp lại tiêu đề 2 lần để tăng trọng số trong embedding\n",
    "                enhanced_text = article_title + \" \" + article_title + \" \" + enhanced_text\n",
    "            \n",
    "            # Tăng cường số khoản, điểm\n",
    "            para_points = re.findall(r'(\\d+\\.\\s*[^\\.\\']+|[a-z]\\)\\s*[^;]+)', enhanced_text)\n",
    "            if para_points:\n",
    "                # Thêm các khoản, điểm vào đầu văn bản để tăng trọng số\n",
    "                para_points_text = \" \".join(para_points[:3])  # Giới hạn 3 khoản/điểm đầu tiên\n",
    "                enhanced_text = para_points_text + \" \" + enhanced_text\n",
    "            \n",
    "            # 2. Tạo embedding cho văn bản đã tăng cường\n",
    "            chunk.embedding = self.model.encode(enhanced_text)\n",
    "        \n",
    "        return chunks\n",
    "    \n",
    "    def _create_hierarchical_embeddings(self, chunks: List[LegalChunk]) -> List[LegalChunk]:\n",
    "        \"\"\"Tạo embedding phân cấp cho các chunk.\"\"\"\n",
    "        \n",
    "        # Tổ chức chunks theo Điều\n",
    "        article_dict = {}\n",
    "        for chunk in chunks:\n",
    "            article_id = chunk.metadata.article_id\n",
    "            if article_id not in article_dict:\n",
    "                article_dict[article_id] = []\n",
    "            article_dict[article_id].append(chunk)\n",
    "        \n",
    "        # Tạo embedding cấp Điều\n",
    "        article_embeddings = {}\n",
    "        for article_id, article_chunks in article_dict.items():\n",
    "            # Ghép tất cả chunk của điều này\n",
    "            full_article_text = \" \".join([chunk.text for chunk in article_chunks])\n",
    "            \n",
    "            # Tạo embedding cấp Điều\n",
    "            article_embeddings[article_id] = self.model.encode(full_article_text)\n",
    "        \n",
    "        # Kết hợp embedding cấp Điều với embedding cấp chunk\n",
    "        for chunk in chunks:\n",
    "            # Tạo embedding cấp chunk\n",
    "            chunk_embedding = self.model.encode(chunk.text)\n",
    "            \n",
    "            # Kết hợp với embedding cấp Điều (với trọng số)\n",
    "            article_embedding = article_embeddings[chunk.metadata.article_id]\n",
    "            combined_embedding = 0.7 * chunk_embedding + 0.3 * article_embedding\n",
    "            \n",
    "            # Chuẩn hóa\n",
    "            norm = np.linalg.norm(combined_embedding)\n",
    "            if norm > 0:\n",
    "                combined_embedding = combined_embedding / norm\n",
    "            \n",
    "            chunk.embedding = combined_embedding\n",
    "        \n",
    "        return chunks\n",
    "\n",
    "    def search(self, query: str, chunks: List[LegalChunk], top_k: int = 3) -> List[Dict]:\n",
    "        \"\"\"Tìm kiếm các chunk liên quan đến truy vấn.\"\"\"\n",
    "        query_embedding = self.model.encode(query)\n",
    "        \n",
    "        results = []\n",
    "        for chunk in chunks:\n",
    "            if chunk.embedding is not None:\n",
    "                # Tính độ tương đồng cosine\n",
    "                similarity = np.dot(query_embedding, chunk.embedding) / (\n",
    "                    np.linalg.norm(query_embedding) * np.linalg.norm(chunk.embedding)\n",
    "                )\n",
    "                \n",
    "                results.append({\n",
    "                    \"chunk\": chunk,\n",
    "                    \"similarity\": float(similarity)\n",
    "                })\n",
    "        \n",
    "        # Sắp xếp theo độ tương đồng\n",
    "        results.sort(key=lambda x: x[\"similarity\"], reverse=True)\n",
    "        \n",
    "        # Chuyển đổi kết quả sang định dạng dễ sử dụng\n",
    "        formatted_results = []\n",
    "        for result in results[:top_k]:\n",
    "            chunk = result[\"chunk\"]\n",
    "            formatted_results.append({\n",
    "                \"text\": chunk.text,\n",
    "                \"metadata\": asdict(chunk.metadata),\n",
    "                \"similarity\": result[\"similarity\"]\n",
    "            })\n",
    "        \n",
    "        return formatted_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Weaviate configuration\n",
    "WEAVIATE_URL=\"https://ekg9amszqaap6mlmw9fixq.c0.us-west3.gcp.weaviate.cloud\"\n",
    "WEAVIATE_API_KEY=\"WZoR5excldEQt0XqH4E2X3zMEvVfvFzr7lc5\"\n",
    "DATA_COLLECTION = \"ND168\"\n",
    "\n",
    "# Device and model configuration\n",
    "DEVICE = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "MODEL_NAME = \"qducnguyen/vietnamese-bi-encoder\"\n",
    "SENTENCE_TRANSFORMER_MODEL = MODEL_NAME  # Use the same model for consistency\n",
    "\n",
    "# Chunking configuration9\n",
    "MAX_CHUNK_TOKENS = 1000  # Optimized for Vietnamese legal text\n",
    "CHUNK_OVERLAP = 300  # Small overlap to maintain context\n",
    "EMBEDDING_METHOD = \"enhanced\"  # Options: basic, enhanced, hierarchical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and Process Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load raw documents\n",
    "with open(\"/home/ltnga/LawVN-Instructction-Gen/src/data/data.json\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Process each document with specialized legal document processing\n",
    "legal_processor = LegalDocument(max_chunk_tokens=MAX_CHUNK_TOKENS, overlap_tokens=CHUNK_OVERLAP)\n",
    "all_chunks = []\n",
    "\n",
    "for doc_text in data:\n",
    "    # Process with legal document chunking\n",
    "    chunks = legal_processor.process(doc_text)\n",
    "    all_chunks.extend(chunks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize embedding generator\n",
    "embedder = LegalEmbedding(model_name=SENTENCE_TRANSFORMER_MODEL)\n",
    "\n",
    "# Generate embeddings with the specified method\n",
    "chunks_with_embeddings = embedder.create_embeddings(all_chunks, method=EMBEDDING_METHOD)\n",
    "\n",
    "# Optional: Apply Vietnamese tokenization for improved search\n",
    "for chunk in chunks_with_embeddings:\n",
    "    # Store original text for display\n",
    "    chunk.metadata.original_text = chunk.text\n",
    "    # Tokenize for better vectorization (Vietnamese-specific)\n",
    "    chunk.tokenized_text = ViTokenizer.tokenize(chunk.text.lower())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Weaviate Vector Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to Weaviate\n",
    "client = weaviate.connect_to_weaviate_cloud(\n",
    "    cluster_url=WEAVIATE_URL,\n",
    "    auth_credentials=Auth.api_key(WEAVIATE_API_KEY),\n",
    ")\n",
    "\n",
    "# Check if collection exists, delete if it does\n",
    "if client.collections.exists(DATA_COLLECTION):\n",
    "    client.collections.delete(DATA_COLLECTION)\n",
    "\n",
    "# Create collection with proper schema using DataType enum\n",
    "collection = client.collections.create(\n",
    "    name=DATA_COLLECTION,\n",
    "    vectorizer_config=None,  # We'll provide our own vectors\n",
    "    properties=[\n",
    "        {\"name\": \"text\", \"data_type\": DataType.TEXT},\n",
    "        {\"name\": \"article_id\", \"data_type\": DataType.TEXT},\n",
    "        {\"name\": \"title\", \"data_type\": DataType.TEXT},\n",
    "        {\"name\": \"type\", \"data_type\": DataType.TEXT},\n",
    "        {\"name\": \"paragraphs\", \"data_type\": DataType.TEXT_ARRAY}\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Index Documents in Weaviate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare batch import\n",
    "with collection.batch.dynamic() as batch:\n",
    "    for i, chunk in enumerate(chunks_with_embeddings):\n",
    "        # Convert paragraphs list to strings for Weaviate compatibility\n",
    "        paragraphs_str = [str(p) for p in chunk.metadata.paragraphs]\n",
    "        \n",
    "        # Add object with its embedding\n",
    "        batch.add_object(\n",
    "            properties={\n",
    "                \"text\": chunk.text,\n",
    "                \"article_id\": chunk.metadata.article_id,\n",
    "                \"title\": chunk.metadata.title,\n",
    "                \"type\": chunk.metadata.type,\n",
    "                \"paragraphs\": paragraphs_str\n",
    "            },\n",
    "            vector=chunk.embedding.tolist() if chunk.embedding is not None else None\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Query Interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_legal_documents(query, top_k=5):\n",
    "    \"\"\"Search for legal documents matching the query.\"\"\"\n",
    "    # Tokenize query for Vietnamese\n",
    "    tokenized_query = ViTokenizer.tokenize(query.lower())\n",
    "    \n",
    "    # Create query embedding using our legal embedder\n",
    "    query_embedding = embedder.model.encode(tokenized_query)\n",
    "    \n",
    "    # Search in Weaviate\n",
    "    results = collection.query.near_vector(\n",
    "        near_vector=query_embedding.tolist(),\n",
    "        limit=top_k,\n",
    "        return_properties=[\"text\", \"article_id\", \"title\", \"type\", \"paragraphs\"]\n",
    "    )\n",
    "    \n",
    "    return results.objects"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result 1:\n",
      "Article ID: 20\n",
      "Text: Điều 20.  1. Phạt tiền từ 100. 000 đồng đến 200. 000 đồng đối với hành vi vi phạm: không hướng dẫn hành khách đứng, nằm, ngồi đúng vị trí quy định trong xe. 2. Phạt tiền từ 400. 000 đồng đến 600. 000 ...\n",
      "\n",
      "================================================\n",
      "Result 2:\n",
      "Article ID: 18\n",
      "Text: Điều 18.  1. Phạt cảnh cáo người từ đủ 14 tuổi đến dưới 16 tuổi điều khiển xe mô tô, xe gắn máy, các loại xe tương tự xe mô tô và các loại xe tương tự xe gắn máy hoặc điều khiển xe ô tô, điều khiển xe...\n",
      "\n",
      "================================================\n",
      "Result 3:\n",
      "Article ID: 12\n",
      "Text: Điều 12.  1. Phạt tiền từ 100. 000 đồng đến 200. 000 đồng đối với người được chở trên xe đạp, xe đạp máy sử dụng ô (dù). 2. Phạt tiền từ 200. 000 đồng đến 250. 000 đồng đối với cá nhân thực hiện một trong các hành vi vi phạm sau đây:\n",
      "...\n",
      "b) Không đội “mũ bảo hiểm cho người đi mô tô, xe máy” hoặc đội “mũ bảo hiểm cho người đi mô tô, xe máy” không cài quai đúng quy cách khi tham gia giao thông trên đường bộ. 6. Phạt tiền từ 500. 000 đồng đến 1.  000. 000 đồng đối với cá nhân, từ 1.  000. 000 đồng đến 2.  000. 000 đồng đối với tổ chức thực hiện một trong các hành vi vi phạm sau đây:\n",
      "\n",
      "================================================\n",
      "Result 4:\n",
      "Article ID: 26\n",
      "Text: Điều 26. (tiếp) 000. 000 đồng đối với tổ chức kinh doanh vận tải thực hiện một trong các hành vi vi phạm sau đây:\n",
      "a) Sử dụng xe vận chuyển động vật sống không có kết cấu phù hợp với loại động vật chuy...\n",
      "\n",
      "================================================\n",
      "Result 5:\n",
      "Article ID: 21\n",
      "Text: Điều 21.  1. Phạt tiền từ 600. 000 đồng đến 800. 000 đồng đối với một trong các hành vi vi phạm sau đây:\n",
      "a) Điều khiển xe xếp hàng trên nóc buồng lái, xếp hàng làm lệch xe;\n",
      "b) Không chốt, đóng cố định...\n",
      "\n",
      "================================================\n"
     ]
    }
   ],
   "source": [
    "# Test the search\n",
    "query = \"Hành vi chở trẻ em dưới 10 tuổi và chiều cao dưới 1,35 mét trên ô tô mà không sử dụng thiết bị an toàn phù hợp sẽ bị phạt tiền\"\n",
    "results = search_legal_documents(query)\n",
    "\n",
    "for i, result in enumerate(results):\n",
    "    print(f\"Result {i+1}:\")\n",
    "    print(f\"Article ID: {result.properties['article_id']}\")\n",
    "    \n",
    "    # Extract and display the relevant part about helmets\n",
    "    text = result.properties[\"text\"]\n",
    "    if \"mũ bảo hiểm\" in text.lower():\n",
    "        # Display the article headline\n",
    "        headline = text.split(\"\\n\")[0] if \"\\n\" in text else text[:100]\n",
    "        print(f\"Text: {headline}\")\n",
    "        \n",
    "        # Find and display paragraphs containing the helmet info\n",
    "        for paragraph in text.split(\"\\n\"):\n",
    "            if \"mũ bảo hiểm\" in paragraph.lower():\n",
    "                print(f\"...\\n{paragraph}\")\n",
    "    else:\n",
    "        # Just show a preview if helmet info isn't explicitly mentioned\n",
    "        print(f\"Text: {text[:200]}...\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 48)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced Search With Specific Reference Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_penalty_info(query, top_k=3):\n",
    "    \"\"\"Extract specific penalty information based on the query.\"\"\"\n",
    "    results = search_legal_documents(query, top_k=top_k)\n",
    "    \n",
    "    penalties = []\n",
    "    for result in results:\n",
    "        text = result.properties[\"text\"]\n",
    "        \n",
    "        # Look for penalty amounts\n",
    "        money_patterns = re.findall(r'phạt tiền từ ([\\d\\.]+) đồng đến ([\\d\\.]+) đồng', text, re.IGNORECASE)\n",
    "        \n",
    "        # Extract relevant behavior descriptions\n",
    "        behaviors = []\n",
    "        for line in text.split(\"\\n\"):\n",
    "            if \"mũ bảo hiểm\" in line.lower() and (\"không đội\" in line.lower() or \"không cài quai\" in line.lower()):\n",
    "                behaviors.append(line.strip())\n",
    "        \n",
    "        # If we found both penalty and behavior\n",
    "        if money_patterns and behaviors:\n",
    "            for min_amount, max_amount in money_patterns:\n",
    "                penalties.append({\n",
    "                    \"article_id\": result.properties[\"article_id\"],\n",
    "                    \"min_amount\": min_amount,\n",
    "                    \"max_amount\": max_amount,\n",
    "                    \"behaviors\": behaviors,\n",
    "                    \"excerpt\": text[:300] + \"...\" if len(text) > 300 else text\n",
    "                })\n",
    "    \n",
    "    return penalties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== MỨC PHẠT TIỀN KHI KHÔNG ĐỘI MŨ BẢO HIỂM ===\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Extract specific penalty information\n",
    "query = \"Không đội mũ bảo hiểm khi đi xe máy mức phạt\"\n",
    "penalty_info = extract_penalty_info(query)\n",
    "\n",
    "# Format the results\n",
    "print(\"=== MỨC PHẠT TIỀN KHI KHÔNG ĐỘI MŨ BẢO HIỂM ===\")\n",
    "print()\n",
    "\n",
    "for penalty in penalty_info:\n",
    "    print(f\"Theo Điều {penalty['article_id']}:\")\n",
    "    print(f\"- Mức phạt: {penalty['min_amount']} đồng đến {penalty['max_amount']} đồng\")\n",
    "    print(\"- Áp dụng cho hành vi:\")\n",
    "    for behavior in penalty['behaviors']:\n",
    "        print(f\"  * {behavior}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Close Weaviate Connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Properly close the Weaviate client connection\n",
    "client.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
